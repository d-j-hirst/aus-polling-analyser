
 -recompose tcp votes for seats (only where they're the same combo as last time, leave other scenarios until later)
 
-16.9695 - priorFpShare
 
 -deviations can be injected into the mean expectation for election/region/seat by simply adding it to the (transformed) values
 -variance is a little more tricky, but can be approximated for now with a sliding linear scale (never exceeding pre-election variance)
  -could use a mix between historically observed variance with complete correlation and historically observed variance with no correlation, with mix factor and "historical observations" just guesstimates for a first pass.
 -observe Nationals ratio (compare to expectations)
 -secondary categories (vote type, urban/rural status, maybe booth size) can then be identified based on patterns in specific deviations. This can be done at whole-election level for a simpler implementation.
 (-doing the above with only vote type as a secondary category probably enoguh for WA election, test on vic/qld elections with any remaining spare time-)
 -rudimentary use of tcp estimates for relevant seats
 
 ## Changes for live-simulations v2.5
 -Resolve situations where TCP progress decreases (and not due to fresh realignment) e.g. Franklin, Griffith. Also address TCP progress being (far) ahead of FP progress for Melbourne.
 -Address overly strong decline in Curtin IND win% and sudden snapback at 8:49 without new results
 -Check Franklin Tuesday 11am low ALP win % if not already resolved
 -Why is Calwell showing 100% ALP on election night (should be some emerging IND change)?
 -TPP progress much less than half of FP progress in some seats (e.g. Brisbane on Tuesday) - why?
 -Franklin Wednesday 12:30pm second IND rated slight chance of winning even though mathematically impossible based on final fps.
 -Why is LIB given such a relatively high chance in Ryan (2025-05-14 12:01) even though ALP win extremely unlikely?
 -Prepoll StdDev is clearly dropping too fast relative to changes in the bias estimate (changes in the bias estimate continue to drop even when the StdDev is ~zero). Check (a) is bias estimate accurate to data (before blending to baseline) (b) does bias estimate adjust too slowly based on 2025 data (c) does StdDev need to be changed to match bias estimate
 -Call of Goldstein at 2025-05-14 13:32 seems aggressive considering final margin
 -in order to better calibrate the balance between baseline and live results, we need to store standard deviations as part of the live baseline, not just medians. These can then be compared with the standard deviation derived from the live results to determine the weightings for baseline vs. live. (Low-medium difficulty, needed for other steps)
 -for live results, calculate standard deviation of remaining results per seat based on theoretical or empirical formula rather than just vibes. Current vibes-based calculation is clearly (based on the smooth curves seen) overly conservative (as all vibes-based results need to be), with proper calibration we can tighten it up and make the live simulation more reactive. Way it should work: we have a formula for calculating the standard deviation of remaining results (in same vote category) based on (a) % of vote counted (b) number of different booths. Perhaps add a variable for diversity in size/location and weightings for confidence in the 2cp/2pp for fp-only booths (Medium-high difficulty, high impact)
 -Variance in estimation of NAT/LIB ratio, even a crude vibes-based variation formula is better than what we have now (Low difficulty, medium impact)
 -increase uncertainty for TPPs in scenarios where different coalition partner enters TCP to the counted matchup (relevant to Bullwinkel/Bendigo 2025) (Low difficulty, low impact)
 -also while we're doing empirical calibration, measure differences between different vote types and establish an empirical calibration for (a) mean degree of reversion to poll-trend tpp (b) standard deviation (look for changes over time) (c) how indicative partial counts are. Then replace vibes-based formulas with actually calibrated ones. (Medium difficulty for (a)/(b), high difficulty for (c), medium-high impact)
 -Extend some more things (like booth/vote type bias handling) from tpp to fp and (at a stretch) even common TCP matchups (need to be careful esp. with independents) (Medium difficulty, low-medium impact impact)
 -possible change (for live result integration): rather than post-adjusting seats to the baseline based on their confidence, adjust stdev in the prior towards zero for each source of variance (overall, region, seat) based on the confidence in the live data; the amount of reduction should be based on the relative confidence levels and be in line with the weight given to the results (which should be more grounded rather than the current ad-hoc formulas). Possible issue: this could change the median from the live-baseline result in incorrect end results; possible solutions (a) use mean instead of median as baseline (would this actually solve the problem? might also create new issues) (b) pre-calculate a correcting factor for the zero-variance case then partially apply it based on the weighting given to the prior in each seat (a large-scale change, but only needs to be calculated once as there's not variation, could be cached like the live-baseline, also might not be accurate with partial result) (Medium-high difficulty, low-medium impact)
 -Naming/functional change: rather than "deviations" all the way, use (a) "deviations" for observed deviations in individual places and aggregates thereof (b) "predictor" for predictions of remaining devations based on the observed deviations - this will include a standard deviation as well as a mean (c) "final deviation" for mean final estimate of how a seat will change from the baseline (d) "factor sensitivity" (maybe change) for additional adjustments based on seat sensitivity to biases in booth type, vote type, etc. (e) "adjustment" for the final adjustment to the baseline based on a specific instance of generated variability
 -Booth "memory" - keep track of previously observed booth results (in a file) for e.g. realigned TCP seats (so that we "remember" the previous result for different TCP scenarios)
 
 
 -Inclusion of known stats for prepoll/postal votes and adjustments between categories
 -Weight confidence heavily based on diversity (downweight similar inputs, upweight diverse inputs, truly high confidence requires all significant categories to be well covered)
 -Actually determine best parameters for variance
 -Modelling of formal vote % of electorate (somehow)
 -change weighting based on booths' prior indicativeness and status (e.g. booths that have changed seats in a redistribution should be downweighted and/or specially categorised)
 -whatever else

 -Rework pollster analysis to strictly exclude future polls and generate separate data for any given date/election combination. (Will create a lot of files, but should be small individually)
 -Setup 6-monthly calibration updates for in-progress polls and set pollster analyses to use these where possible
 -Run all poll trends that have enough previous pollster data (assess how much after completing previous points)
 -(Wait for above to complete)
 -Rework trend adjust to strictly avoid using future elections and re-run for all elections past a certain date (this creates the baseline for comparison with the next step)
 -Overhaul trend adjust to use bayesian aggregation to determine best bias adjustment for each party type

 -General refactor of frontend as there is currently a lot of code duplication
 -Develop mechanism for making hindcasts and assigning an appropriate date for when they "would" have been done
 -Check that all parts of the system sufficiently exclude an election from impacting its own forecast (for hindcasts)
 -Overhaul mechanisms to count WA Nats as a separate party while maintaining TPP dynamics
 -Create hindcasts for earlier parts of 2022fed and maybe 2022sa
 -Create hindcasts for earlier elections
 
Workflow:
 -run trend models with --bias, --calibrate
 -run pollster analyses (must wait for all trend models with overlapping time to do --bias and --calibrate)
 -run "pure" analyses (pollster analysis for this election (only) must be complete)
 -run full analyses (pure analysis for all previous and overlapping times must be complete)
 -run analyses w/cutoffs (can be done as soon as previous/overlapping pure trends are done)
 -run trend adjustment (need cutoffs up to and including this election to be complete)

-glossary
 -informal/formal votes
 -retirement
 -independent
 -majority
 -minority
 -Coalition
 -forecast
 -projection
 -prediction
 -probabilistic forecast
 -probability
 -government
 -lower house
 -region
 -scenario
 -election
 -state
 -territory
 -others
 -undecided
 -margin of error
 -hindcast

